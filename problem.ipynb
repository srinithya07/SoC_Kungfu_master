{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Arm:\n",
    "    def __init__(self, p):\n",
    "        self.p = p\n",
    "    \n",
    "    def pull(self):\n",
    "        return np.random.binomial(1, self.p)\n",
    "\n",
    "class MultiBandit:\n",
    "    def __init__(self, probs = [0.1, 0.2, 0.7, 0.5]):\n",
    "        self.__arms = [Arm(p) for p in probs]\n",
    "        self.__regret = 0\n",
    "        self.__maxp = max(probs)\n",
    "\n",
    "    def num_arms(self):\n",
    "        return len(self.__arms)\n",
    "\n",
    "    def pull(self, arm_num):\n",
    "        reward = self.__arms[arm_num].pull()\n",
    "        self.__regret += self.__maxp-self.__arms[arm_num].p\n",
    "        return reward\n",
    "    \n",
    "    def regret(self):\n",
    "        return self.__regret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedyAlgorithm:\n",
    "    def __init__(self, num_arms, horizon, epsilon):\n",
    "        # Initialize our algorithm\n",
    "        self.num_arms = num_arms                    # Number of arms present in the bandit\n",
    "        self.horizon = horizon                      # Total Horizon of the algorithm\n",
    "        self.epsilon = epsilon                      # epsiolon value for the algorithm\n",
    "        self.timestep = 0                           # The current timestep while running the algorithm\n",
    "        self.arm_pulls = np.zeros(num_arms)         # History of number of times each arm was pulled\n",
    "        self.arm_rewards = np.zeros(num_arms)       # History of the total reward accumulated by each arm\n",
    "        self.regrets = np.zeros(horizon)            # Total regret at each timestep of the horizon\n",
    "\n",
    "    def give_best_arm(self):\n",
    "        # Return the arm which the algorithm considers to be the best arm at end of algorithm\n",
    "        \n",
    "        avg_rewards = np.divide(self.arm_rewards, self.arm_pulls, out=np.zeros_like(self.arm_rewards), where=self.arm_pulls!=0)\n",
    "        return np.argmax(avg_rewards)\n",
    "    def select_arm(self):\n",
    "        # Select arm at each time step. You are supposed to return the index of which \n",
    "        # arm has been selected to pull at this timestep\n",
    "       \n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.randint(0, self.num_arms)\n",
    "        else:\n",
    "            avg_rewards = np.divide(self.arm_rewards, self.arm_pulls, out=np.zeros_like(self.arm_rewards), where=self.arm_pulls!=0)\n",
    "            return np.argmax(avg_rewards)\n",
    "    def run_algorithm(self, bandit):\n",
    "        # This is the proper algorithm. Already completed\n",
    "        for _ in range(self.horizon):\n",
    "            arm_to_pull = self.select_arm()             # Select the arm using the algorithm\n",
    "            reward = bandit.pull(arm_to_pull)           # Pull the arm and find our the reward\n",
    "            self.arm_pulls[arm_to_pull] += 1            # Update the arm pull count and arm reward count\n",
    "            self.arm_rewards[arm_to_pull] += reward\n",
    "            self.timestep += 1                          # Update the timestep\n",
    "            self.regrets[_] = bandit.regret()           # Store the regret values at each timestep\n",
    "    \n",
    "    def plot(self):\n",
    "        # Plot the regret graph. Label the X and Y Axis properly using matplotlib library\n",
    "        pass\n",
    "        plt.figure(figsize=(8,4))\n",
    "        plt.plot(self.regrets)\n",
    "        plt.title(\"Epsilon-Greedy: Total Regret vs Timesteps\")\n",
    "        plt.xlabel(\"Timestep\")\n",
    "        plt.ylabel(\"Total Regret\")\n",
    "        plt.grid(True)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UCBAlgorithm:\n",
    "    def __init__(self, num_arms, horizon):\n",
    "        self.num_arms = num_arms\n",
    "        self.horizon = horizon\n",
    "        self.timestep = 0\n",
    "        self.arm_pulls = np.zeros(num_arms)\n",
    "        self.arm_rewards = np.zeros(num_arms)\n",
    "        self.regrets = np.zeros(horizon)\n",
    "    def give_best_arm(self):\n",
    "        average_rewards = self.arm_rewards / np.maximum(self.arm_pulls, 1)\n",
    "        return int(np.argmax(average_rewards))\n",
    "    def select_arm(self):\n",
    "        if self.timestep < self.num_arms:\n",
    "            return self.timestep\n",
    "        else:\n",
    "            total_counts = self.timestep\n",
    "            average_rewards = self.arm_rewards / self.arm_pulls\n",
    "            confidence_bounds = np.sqrt((2*np.log(total_counts)) / self.arm_pulls)\n",
    "            ucb_values = average_rewards + confidence_bounds\n",
    "            return int(np.argmax(ucb_values))     \n",
    "    def run_algorithm(self, bandit):\n",
    "        for t in range(self.horizon):\n",
    "            arm_to_pull = self.select_arm()\n",
    "            reward = bandit.pull(arm_to_pull)\n",
    "            self.arm_pulls[arm_to_pull] += 1\n",
    "            self.arm_rewards[arm_to_pull] += reward\n",
    "            self.timestep += 1\n",
    "            self.regrets[t] = bandit.regret()\n",
    "    def plot(self):\n",
    "        plt.figure(figsize=(8,4))\n",
    "        plt.plot(self.regrets, label=\"UCB\")\n",
    "        plt.xlabel(\"Timestep\")\n",
    "        plt.ylabel(\"Total Regret\")\n",
    "        plt.title(\"UCB: Total Regret vs Timestep\") \n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThompsonSamplingAlgorithm:\n",
    "    def __init__(self, num_arms, horizon):\n",
    "        self.num_arms = num_arms\n",
    "        self.horizon = horizon\n",
    "        self.timestep = 0\n",
    "        self.successes = np.zeros(num_arms)\n",
    "        self.failures = np.zeros(num_arms)\n",
    "        self.regrets = np.zeros(horizon)\n",
    "    def give_best_arm(self):\n",
    "        beta_means = self.successes / (self.successes + self.failures + 1e-6)\n",
    "        return int(np.argmax(beta_means))\n",
    "    def select_arm(self):\n",
    "        samples = [np.random.beta(self.successes[i] + 1, self.failures[i] + 1) for i in range(self.num_arms)] \n",
    "        return int(np.argmax(samples))\n",
    "    def run_algorithm(self, bandit):\n",
    "        for t in range(self.horizon):\n",
    "            arm_to_pull = self.select_arm()\n",
    "            reward = bandit.pull(arm_to_pull)\n",
    "            if reward == 1:\n",
    "                self.successes[arm_to_pull] += 1\n",
    "            else:\n",
    "                self.failures[arm_to_pull] +=1\n",
    "            self.timestep +=1\n",
    "            self.regrets[t] = bandit.regret()  \n",
    "    def plot(self):\n",
    "        plt.figure(figsize=(8,4))\n",
    "        plt.plot(self.regrets, label=\"Thompson Sampling\")\n",
    "        plt.xlabel(\"Timestep\")\n",
    "        plt.ylabel(\"Total Regret\")\n",
    "        plt.title(\"Thompson Sampling: Total Regret vs Timestep\")\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.show()                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_algorithms(custom_probs, horizon, epsilon=0.1):\n",
    "    print(f\"\\n Evaluating on custom bandit: {custom_probs}\")\n",
    "    bandit1 = MultiBandit(custom_probs)\n",
    "    bandit2 = MultiBandit(custom_probs)\n",
    "    bandit3 = MultiBandit(custom_probs)\n",
    "    algo1 = EpsilonGreedyAlgorithm(num_arms=len(custom_probs), horizon=horizon, epsilon=epsilon)\n",
    "    algo2 = UCBAlgorithm(num_arms=len(custom_probs), horizon=horizon)\n",
    "    algo3 = ThompsonSamplingAlgorithm(num_arms=len(custom_probs),horizon=horizon)\n",
    "    algo1.run_algorithm(bandit1)\n",
    "    algo2.run_algorithm(bandit2)\n",
    "    algo3.run_algorithm(bandit3)\n",
    "    print(f\"\\n Epsilon-Greedy (e={epsilon}): Total Regret = {bandit1.regret():.2f}, Best Arm = {algo1.give_best_arm()}\")\n",
    "    print(f\"UCB : Total Regret = {bandit2.regret():.2f}, Best Arm = {algo2.give_best_arm()}\")\n",
    "    print(f\"Thompson Sampling: Total Regret = {bandit3.regret():.2f}, Best Arm = {algo3.give_best_arm()}\")\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.plot(algo1.regrets, label=f\"Epsilon-Greedy (e={epsilon})\")\n",
    "    plt.plot(algo2.regrets, label=\"UCB\")\n",
    "    plt.plot(algo3.regrets, label=\"Thompson Sampling\")\n",
    "    plt.xlabel(\"Timestep\")\n",
    "    plt.ylabel(\"Total Regret\")\n",
    "    plt.title(\"Total Regret vs Timestep for all Algorithms\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    regrets = {\n",
    "        f\"Epsilon-Greedy (e={epsilon})\": bandit1.regret(),\n",
    "        \"UCB\": bandit2.regret(),\n",
    "        \"Thompson Sampling\": bandit3.regret()\n",
    "    }\n",
    "    worst_algo = max(regrets, key=regrets.get)\n",
    "    print(f\"Based on the current run, the least effective algorithm is: {worst_algo} (Regret = {regrets[worst_algo]:.2f})\")\n",
    "\n",
    "custom_probs = [0.1, 0.4, 0.8, 0.3] #changeable\n",
    "horizon = 100 #set the horizon size\n",
    "evaluate_algorithms(custom_probs, horizon=horizon, epsilon=0.2)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
